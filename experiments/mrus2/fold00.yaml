dataset:
  dir:
    train:
      - "/home/yipenghu/Scratch/data/mrusv2/normalised/deepreg/fold01"
      - "/home/yipenghu/Scratch/data/mrusv2/normalised/deepreg/fold02"
      - "/home/yipenghu/Scratch/data/mrusv2/normalised/deepreg/fold03"
      - "/home/yipenghu/Scratch/data/mrusv2/normalised/deepreg/fold04"
      - "/home/yipenghu/Scratch/data/mrusv2/normalised/deepreg/fold05"
      - "/home/yipenghu/Scratch/data/mrusv2/normalised/deepreg/fold06"
      - "/home/yipenghu/Scratch/data/mrusv2/normalised/deepreg/fold07"
      - "/home/yipenghu/Scratch/data/mrusv2/normalised/deepreg/fold08"
      - "/home/yipenghu/Scratch/data/mrusv2/normalised/deepreg/fold09"
      - "/home/yipenghu/Scratch/data/mrusv2/normalised/deepreg/fold10"
      - "/home/yipenghu/Scratch/data/mrusv2/normalised/deepreg/fold11"
    valid:
      - "/home/yipenghu/Scratch/data/mrusv2/normalised/deepreg/fold00"
    test:
      - "/home/yipenghu/Scratch/data/mrusv2/normalised/deepreg/fold00"
  format: "h5"
  type: "paired"
  labeled: true
  moving_image_shape: [120, 128, 128]
  fixed_image_shape: [81, 118, 88]
train:
  # define neural network structure
  method: "ddf" # the registration method, value should be ddf / dvf / conditional
  backbone:
    name: "local" # value should be local / global / unet
    num_channel_initial: 32 # number of initial channel in local net, controls the size of the network
    extract_levels: [0, 1, 2, 3]

  # define the loss function for training
  loss:
    image:
      name: "ssd"
      weight: 0
    label:
      weight: 1.0
      name: "dice"
      scales: [0, 1, 2, 4, 8, 16, 32]
    regularization:
      weight: 0.5 # weight of regularization loss
      name: "bending" # options include "bending", "gradient"

  # define the optimizer
  optimizer:
    name: "adam" # value should be adam / sgd / rms
    adam:
      learning_rate: 1.0e-5

  # define the hyper-parameters for preprocessing
  preprocess:
    data_augmentation:
      name: "affine"
    batch_size: 4
    shuffle_buffer_num_batch: 1 # shuffle_buffer_size = batch_size * shuffle_buffer_num_batch

  # other training hyper-parameters
  epochs: 20000 # number of training epochs
  save_period: 1000 # the model will be saved every `save_period` epochs.
